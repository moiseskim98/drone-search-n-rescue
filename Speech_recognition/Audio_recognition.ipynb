{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bb609f7-bce4-45dd-80a2-4228fa50fe81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0 Microsoft Sound Mapper - Input, MME (2 in, 0 out)\n",
      ">  1 éº¥å…‹é¢¨æŽ’åˆ— (Realtek High Definition , MME (2 in, 0 out)\n",
      "   2 Microsoft Sound Mapper - Output, MME (0 in, 2 out)\n",
      "<  3 å–‡å­ (Realtek High Definition Aud, MME (0 in, 6 out)\n",
      "   4 Headphones (), Windows WDM-KS (0 in, 2 out)\n",
      "   5 Microphone Array (Realtek HD Audio Mic Array input), Windows WDM-KS (2 in, 0 out)\n",
      "   6 Headphones 1 (Realtek HD Audio 2nd output with SST), Windows WDM-KS (0 in, 2 out)\n",
      "   7 Headphones 2 (Realtek HD Audio 2nd output with SST), Windows WDM-KS (0 in, 6 out)\n",
      "   8 PC Speaker (Realtek HD Audio 2nd output with SST), Windows WDM-KS (2 in, 0 out)\n",
      "   9 Headset Microphone (Realtek HD Audio Mic input), Windows WDM-KS (2 in, 0 out)\n",
      "  10 Speakers 1 (Realtek HD Audio output with SST), Windows WDM-KS (0 in, 2 out)\n",
      "  11 Speakers 2 (Realtek HD Audio output with SST), Windows WDM-KS (0 in, 6 out)\n",
      "  12 PC Speaker (Realtek HD Audio output with SST), Windows WDM-KS (2 in, 0 out)\n",
      "  13 Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free AG Audio%0\n",
      ";(ðŸ¦»Â©ï¸)), Windows WDM-KS (0 in, 1 out)\n",
      "  14 Headset (@System32\\drivers\\bthhfenum.sys,#2;%1 Hands-Free AG Audio%0\n",
      ";(ðŸ¦»Â©ï¸)), Windows WDM-KS (1 in, 0 out)\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "\n",
    "# List all available devices\n",
    "devices = sd.query_devices()\n",
    "\n",
    "# Print the list of devices\n",
    "print(devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e689d84d-3c84-4234-91cd-34e04db5e3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lltwdks\\anaconda3\\Lib\\site-packages\\librosa\\core\\pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  return pitch_tuning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No\\no400.wav is silent or too quiet. Skipping chroma extraction.\n",
      "Model training completed and saved as 'yes_no_classifier.joblib'\n",
      "Cross-validation scores: [0.9382716  0.94444444 0.94444444 0.94409938 0.88198758]\n",
      "Mean cross-validation score: 0.93\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "\n",
    "# Paths to folders containing \"yes\" and \"no\" audio samples\n",
    "yes_path = 'Yes'\n",
    "no_path = 'No'\n",
    "\n",
    "def is_silent(audio, threshold=0.02):\n",
    "    \"\"\"Check if the audio is mostly silent based on a threshold.\"\"\"\n",
    "    return np.max(np.abs(audio)) < threshold\n",
    "\n",
    "\n",
    "# Function to extract MFCC features from an audio file\n",
    "def extract_features(file_path):\n",
    "    audio, sr = librosa.load(file_path, sr=16000)\n",
    "    \n",
    "    # Check if the audio is silent to avoid errors in chroma feature extraction\n",
    "    if is_silent(audio):\n",
    "        print(f\"Warning: {file_path} is silent or too quiet. Skipping chroma extraction.\")\n",
    "        chroma_mean = np.zeros(12)  # Filler for chroma features if audio is silent\n",
    "    else:\n",
    "        # Extract Chroma features only if audio is not silent\n",
    "        chroma = librosa.feature.chroma_stft(y=audio, sr=sr)\n",
    "        chroma_mean = np.mean(chroma.T, axis=0)\n",
    "\n",
    "    # Extract MFCC and Spectral Contrast features\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=5)\n",
    "    mfccs_mean = np.mean(mfccs.T, axis=0)\n",
    "    \n",
    "    spectral_contrast = librosa.feature.spectral_contrast(y=audio, sr=sr)\n",
    "    spectral_contrast_mean = np.mean(spectral_contrast.T, axis=0)\n",
    "    \n",
    "    # Concatenate all features\n",
    "    features = np.concatenate([mfccs_mean, spectral_contrast_mean, chroma_mean])\n",
    "    return features\n",
    "\n",
    "# Prepare data and labels\n",
    "X = []  # Feature vectors\n",
    "y = []  # Labels\n",
    "\n",
    "# Process \"yes\" files\n",
    "for file_name in os.listdir(yes_path):\n",
    "    if file_name.endswith('.wav'):\n",
    "        file_path = os.path.join(yes_path, file_name)\n",
    "        features = extract_features(file_path)\n",
    "        X.append(features)\n",
    "        y.append(1)  # Label for \"yes\"\n",
    "\n",
    "# Process \"no\" files\n",
    "for file_name in os.listdir(no_path):\n",
    "    if file_name.endswith('.wav'):\n",
    "        file_path = os.path.join(no_path, file_name)\n",
    "        features = extract_features(file_path)\n",
    "        X.append(features)\n",
    "        y.append(0)  # Label for \"no\"\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest Classifier with additional regularization\n",
    "clf = RandomForestClassifier(\n",
    "    n_estimators=10,\n",
    "    max_depth=2,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    random_state=42,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(clf, 'yes_no_classifier.joblib')\n",
    "print(\"Model training completed and saved as 'yes_no_classifier.joblib'\")\n",
    "\n",
    "# Evaluate with cross-validation\n",
    "#cross_val_scores = cross_val_score(clf, X, y, cv=5)\n",
    "#print(f\"Cross-validation scores: {cross_val_scores}\")\n",
    "#print(f\"Mean cross-validation score: {cross_val_scores.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52bec88c-890a-498d-b4aa-3a1c7f27082c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 91.98%\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          No       0.91      0.92      0.92        77\n",
      "         Yes       0.93      0.92      0.92        85\n",
      "\n",
      "    accuracy                           0.92       162\n",
      "   macro avg       0.92      0.92      0.92       162\n",
      "weighted avg       0.92      0.92      0.92       162\n",
      "\n",
      "Confusion Matrix:\n",
      " [[71  6]\n",
      " [ 7 78]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the trained model\n",
    "clf = joblib.load('yes_no_classifier.joblib')\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred = clf.predict(X_val)\n",
    "\n",
    "# Calculate and print validation accuracy\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "print(\"Validation Accuracy: {:.2f}%\".format(val_accuracy * 100))\n",
    "\n",
    "# Print detailed validation report\n",
    "print(\"Classification Report:\\n\", classification_report(y_val, y_val_pred, target_names=[\"No\", \"Yes\"]))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_val, y_val_pred))\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_val, y_val_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "799d2b28-b28f-4447-8586-2fcc63b63c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start detecting program definition part\n",
    "\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from scipy.io.wavfile import write\n",
    "import os\n",
    "\n",
    "def predict_yes_no_from_audio(model, duration=3, fs=44100, device_index=5):\n",
    "    try:\n",
    "        # Record audio\n",
    "        print(\"Recording...\")\n",
    "        myrecording = sd.rec(int(duration * fs), samplerate=fs, channels=2, dtype=np.float32)\n",
    "        sd.wait()  # Wait until recording is finished\n",
    "        print(\"Recording finished.\")\n",
    "        \n",
    "        # Save the recorded audio to a temporary file\n",
    "        temp_file = 'temp_audio.wav'\n",
    "        write(temp_file, fs, myrecording)\n",
    "        \n",
    "        # Extract features using the temp audio file path\n",
    "        features = extract_features(temp_file)\n",
    "        \n",
    "        # Predict using the model\n",
    "        prediction = model.predict([features])\n",
    "        \n",
    "        # Delete the temporary file if not needed anymore\n",
    "        os.remove(temp_file)\n",
    "        \n",
    "        return \"Yes\" if prediction[0] == 1 else \"No\"\n",
    "\n",
    "    except sd.PortAudioError as e:\n",
    "        print(f\"Error during recording: {e}\")\n",
    "        print(\"Please ensure an audio input device is connected and enabled.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "# Example usage of the function\n",
    "#loaded_model = joblib.load('yes_no_classifier.joblib')\n",
    "\n",
    "#result = predict_yes_no_from_audio(loaded_model)\n",
    "#if result:\n",
    "#    print(f\"You said: {result}\")\n",
    "\n",
    "\n",
    "def adjust():\n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Adjusting for ambient noise...\")\n",
    "        recognizer.adjust_for_ambient_noise(source, duration=1)  # Adjust for ambient noise\n",
    "\n",
    "def speechrecognize():\n",
    "    # Create a Recognizer instance\n",
    "    recognizer = sr.Recognizer()\n",
    "    # Capture audio input from the microphone\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Recording...\")\n",
    "        audio_data = recognizer.listen(source, phrase_time_limit=30)  # Listen to the audio from the microphone\n",
    "        print(\"Finished recording.\")  # This will print after the recording is done    \n",
    "    # You can now process the audio_data, for example:\n",
    "    try:\n",
    "        # Recognize speech using Google's recognition\n",
    "        text = recognizer.recognize_google(audio_data)\n",
    "        print(\"You said:\", text)\n",
    "        return text\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Could not understand the audio\")\n",
    "    except sr.RequestError as e:\n",
    "        print(f\"Could not request results; {e}\")\n",
    "\n",
    "def switch(order):\n",
    "    if order == 1:\n",
    "        sentence = \"Hello! Do you need any help?\"\n",
    "        engine.say(sentence)\n",
    "        engine.runAndWait()\n",
    "        loaded_model = joblib.load('yes_no_classifier.joblib')\n",
    "        return predict_yes_no_from_audio(loaded_model)\n",
    "        \n",
    "    elif order == 2:\n",
    "        sentence = \"Okay, What do you need?\"\n",
    "        engine.say(sentence)\n",
    "        engine.runAndWait()\n",
    "        return speechrecognize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b19d7a7b-45d3-4a10-b39c-9a14d387d16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting for ambient noise...\n",
      "Recording...\n",
      "Recording finished.\n",
      "You said: Yes\n",
      "Recording...\n",
      "Finished recording.\n",
      "You said: I need water food and a helicopter\n",
      "Results saved to results.csv\n"
     ]
    }
   ],
   "source": [
    "import pyttsx3\n",
    "import speech_recognition as sr\n",
    "import csv\n",
    "\n",
    "# Initialize the TTS engine\n",
    "engine = pyttsx3.init()\n",
    "\n",
    "# Set properties (optional)\n",
    "engine.setProperty('rate', 260)  # Speed of speech\n",
    "engine.setProperty('volume', 1)  # Volume level (0.0 to 1.0)\n",
    "\n",
    "# Adjust for ambient noise before recording\n",
    "adjust()\n",
    "    \n",
    "# Create a new CSV file to store the results\n",
    "csv_file_path = 'results.csv'\n",
    "\n",
    "# Open the CSV file in write mode\n",
    "with open(csv_file_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    # Get the result of switch 1\n",
    "    text1 = switch(1)\n",
    "    print (f\"You said: {text1}\")\n",
    "\n",
    "    # Write the result of switch 1 to the first row\n",
    "    writer.writerow(['Need help?', text1])\n",
    "\n",
    "    # Check the response of switch 1 and proceed to switch 2 if needed\n",
    "    if text1 and text1.lower() == 'yes':\n",
    "        text2 = switch(2)\n",
    "    else:\n",
    "        text2 = 'Not Applicable'  # If the answer is \"no\", don't ask further questions\n",
    "\n",
    "    # Write the result of switch 2 to the second row\n",
    "    writer.writerow(['What the person needs.', text2])\n",
    "\n",
    "print(f\"Results saved to {csv_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dab58f06-8559-44d3-9d38-7b43667b3371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Need help?                                 Yes\n",
      "0  What the person needs.  I need water food and a helicopter\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the CSV file\n",
    "csv_file_path = 'results.csv'\n",
    "\n",
    "# Load CSV into a DataFrame and print\n",
    "df = pd.read_csv(csv_file_path)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87a5232-44ef-4aea-8540-174e7b37342c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the accuracy and numbers of trees\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_estimators_range = range(1, 50, 1)  \n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for n_estimators in n_estimators_range:\n",
    "    \n",
    "    clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=2, min_samples_split=10, \n",
    "                                 min_samples_leaf=5, random_state=42, class_weight='balanced')\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    \n",
    "    train_accuracy = accuracy_score(y_train, clf.predict(X_train))\n",
    "    val_accuracy = accuracy_score(y_val, clf.predict(X_val))\n",
    "    \n",
    "   \n",
    "    train_accuracies.append(train_accuracy)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(n_estimators_range, train_accuracies, label=\"Training Accuracy\")\n",
    "plt.plot(n_estimators_range, val_accuracies, label=\"Validation Accuracy\")\n",
    "plt.xlabel(\"Number of Trees (n_estimators)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "#plt.title(\"Accuracy vs. Number of Trees in Random Forest\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
